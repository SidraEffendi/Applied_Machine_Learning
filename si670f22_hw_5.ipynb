{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OWTe9VS3_b11"
   },
   "source": [
    "## SI 670 Applied Machine Learning, Week 5:  One-hot encoding, calibration, decision trees, random forests, data leakage. (Due Wednesday October 5 2022, 11:59pm.)\n",
    "\n",
    "Total: 100 points\n",
    "Question 1: 30 points\n",
    "Question 2: 20 points\n",
    "Question 3: 30 points \n",
    "Question 4: 20 points\n",
    "\n",
    "* This homework is worth 100 points in total. Correct answers and code receive full credit, but partial credit will be awarded if you have the right idea even if your final answers aren't quite right.\n",
    "\n",
    "* Submit your completed notebook file to the Canvas site - **IMPORTANT**: please name your submitted file `si670f22-hw5-youruniqname.ipynb`\n",
    "\n",
    "* Any file submitted after the deadline will be marked as late. Please consult the syllabus regarding late submission policies. You can submit the homework as many time as you want, but only your latest submission will be graded.\n",
    "\n",
    "* As a reminder, the notebook code you submit must be your own work. Feel free to discuss general approaches to the homework with classmates. If you end up forming more of a team discussion on multiple questions, please include the names of the people you worked with at the top of your notebook file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8E6z1mGZ3GU1"
   },
   "source": [
    "### Question 1 (30 points)\n",
    "\n",
    "This question doesn't require coding, in the sense of writing programs that do all the computing of the answer for you: you can figure out the answer by hand and then put your answers into the notebook by defining the correct numpy array in python. Alternatively you could write out your answers in markdown using simple LaTeX tags. (See [here](https://csrgxtu.github.io/2015/03/20/Writing-Mathematic-Fomulars-in-Markdown/) for how to write things with math operators in LaTeX format.)\n",
    "\n",
    "\n",
    "#### (a) (10 points) One-hot encoding\n",
    "\n",
    "If you have a dataset with three data points, and each data point has three features. Among them, X2 and X3 are categorical variables:\n",
    "\n",
    "|    X1\t|  X2 \t|  X3\t|\n",
    "|----\t|----\t|----\t|\n",
    "|   1.3\t|  a \t| a \t|\n",
    "|   0.7 |  b \t| c \t|\n",
    "|   2.1 |  a \t| b     |\n",
    "\n",
    "X2 can take up the values a and b, and X3 can take up the values a, b and c.\n",
    "\n",
    "Please manually convert this dataset into numerical format with the categorical variables transformed into one-hot encoding. Please keep the order of X1, X2, and X3, and use alphabetical order for the one-hot encoding.\n",
    "\n",
    "\n",
    "#### (b) (10 points) Calibration\n",
    "Recall the calibration curve has the predicted probability as its x-axis and the true probability (also known as the \"empirical probability\" as its y-axis. Suppose you are given a binary classifier and its predicted probabilities on a test set with 15 data points. The labels of these data points are also given. Please calculate the true probabilities in three bins: \\[0, 0.3), \\[0.3, 0.7), \\[0.7, 1\\]. You could further use these probabilities to draw a calibration curve but it's not required for this question. You only need to give the 3 numbers indicating the true probabilities for each bin.  It might help to recall the weather example from class: the \"true\" empirical probability of rain is just the fraction of times it *actually* rained according to the data, for a given predicted probability (or range of probabilities) from the weatherperson.\n",
    "\n",
    "|Predicted probability | Label |\n",
    "|----\t               |----   |\n",
    "|   0.40               |   0   |\n",
    "|   0.77               |   1   |\n",
    "|   0.84               |   0   |\n",
    "|   0.68               |   0   |\n",
    "|   0.73               |   1   |\n",
    "|   0.88               |   1   |\n",
    "|   0.69               |   0   |\n",
    "|   0.24               |   0   |\n",
    "|   0.70               |   1   |\n",
    "|   0.41               |   1   |\n",
    "|   0.34               |   1   |\n",
    "|   0.18               |   1   |\n",
    "|   0.31               |   1   |\n",
    "|   0.58               |   1   |\n",
    "|   0.00               |   0   |\n",
    "\n",
    "\n",
    "#### (c) (10 points) Random forest parameters\n",
    "\n",
    "Suppose your current random forest classifier is facing an overfitting situation. Please state whether increasing or decreasing the following parameters can potentially help you reduce overfitting or not, and why?\n",
    "\n",
    "(i) `n_estimators`\n",
    "\n",
    "(ii) `max_features`\n",
    "\n",
    "(iii) `max_depth`\n",
    "\n",
    "(iv) `n_jobs`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) \n",
    "\n",
    "\n",
    "\n",
    "|    X1\t|  X2_a\t|  X2_b\t|  X3_a\t|  X3_b\t|  X3_c\t|\n",
    "|----\t|----\t|----\t|----\t|----\t|----\t|\n",
    "|   1.3\t|  1 \t| 0 \t|   1\t|  0 \t| 0 \t|\n",
    "|   0.7 |  0 \t| 1 \t|   0\t|  0 \t| 1 \t|\n",
    "|   2.1 |  1 \t| 0     |   0\t|  1 \t| 0     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)\n",
    "\n",
    "For label 1 as positive: E(Y_i=1|H(X_i)= p) \n",
    "\n",
    "[0, 0.3) : 1/3\n",
    "\n",
    "[0.3, 0.7) : 4/7\n",
    "\n",
    "[0.7, 1) : 4/5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### (c)\n",
    "\n",
    "(i) `n_estimators`: Increasing the value of this parameters reduces overfitting for larger datasets because on a small dataset there is only so many diferent trees we can fit till the prediction becomes stable but for a larger dataset the prediction is an average of the result produced by the different trees and hence, even if a individual tree is overfitting, the result is derived from combining result from all different trees. So, more the no.of decision trees more stable will be our prediction.\n",
    "\n",
    "(ii) `max_features`: We want max-features to not be very small not a be the whole n-features because an optimum value will allow for the trees to be diverse, thus making the average prediction more meaningful. If we use very less features that might lead to inefficient tree (not able to partition properly) and if we use all the features, we will get similar trees and mit can lead to overfitting. So, the relationship is non-linear.\n",
    "\n",
    "(iii) `max_depth` : Max depth does not effect overfitting in case of random forests unlike decision trees, because even if one tree overfits not all of them would, so the average prediction does not result in overfitting. \n",
    "\n",
    "(iv) `n_jobs`: This parameter is used for parallelization and does not effect overfitting in any way. More processors which just make the model run faster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9Qlj8Dm3GU5"
   },
   "source": [
    "### Question 2 (20 points) Cross-validation for very small datasets.\n",
    "\n",
    "In our lecture about data leakage, we talked about one simple strategy to help you avoid data leakage: before you do any work with a new dataset, split off a final test dataset, and use this final test dataset as the very last step in your validation. However, when you have a very small dataset, one issue is that it leads to really tiny test sets, which leads to unreliable test evaluation scores. For example, if you have a dataset with a total of 60 samples, and hold back 25% as a final test set, you'll get a final test set with 15 samples. In this case, a single evaluation score based on merely 15 samples could be very unreliable and probably not something to be relied on heavily.\n",
    "\n",
    "To make evaluation more reliable, we discussed how people usually use *cross-validation* to generate *multiple* evaluation scores, each on a different train/test split of the data. That is, you split the train and test set multiple times and then calculate the average of the resulting test scores. This is the approach we'll use to estimate a more reliable final test set score.  We don't want to use these final test sets to also tune our hyperparameters (to avoid data leakage), so we make sure to learn the model and tune the hyperparameters using only the data in the training split.  To do that, we do a second split *within the training data split* so that we have (i) a training set for the model and (ii) a separate *validation split* that's used to evaluate the model and pick the best setting for the hyperparameters.\n",
    "\n",
    "Here's the general recipe:\n",
    "\n",
    "1. Split the whole dataset into $k$ equal folds\n",
    "2. For $i$ from 1 to $k$   (for each of the $k$ folds)\n",
    "\n",
    "    a. Take the $i$-th fold as a final test set. \n",
    "    \n",
    "    b. With the remaining data (i.e. combining the other folds), apply a standard train/test split (75%/25%).\n",
    "    \n",
    "    c. For each possible tuning value of hyperparameter (in our case, alpha):\n",
    "    \n",
    "       - Train the model with the 75% part using a specific hyperparameter choice (for alpha)\n",
    "             \n",
    "       - Evaluate the model with the 25% part. This 25% is our \"validation set\".\n",
    "             \n",
    "       Pick the hyperparameter value that gave the best score on the validation set.\n",
    "            \n",
    "    d. Once you find the optimal hyperparameter, do the test set predictions on this $i$-th fold and calculate the test score for this $i$-th fold.\n",
    "  \n",
    "3. Report the average of the final test set scores you got across all $k$ folds.\n",
    "\n",
    "To simulate a small dataset scenario, we have provided the code that selects the first 60 samples from the built-in boston dataset.. We've also given you the variable 'alpha_list', which has the range of ridge regression hyperparameter alphe you should use for tuning. \n",
    "\n",
    "Write the code that implements the above scheme on this subset of the boston dataset. You can split the whole dataset into k folds (step 1) by using the handy `KFold` function (https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html). For each fold, you can then use the default `train_test_split` to get that's fold's training and validation data (step 2b). With this training data, train a Ridge regression model, and use the validation set to evaluate, tuning to find the optimal hyper-parameter alpha value within each fold (step 2c). Get the final test set score using this optimal model (step 2d). Repeat for all $k$ folds to obtain a set of final test scores.\n",
    "\n",
    "Finally, you need to return the mean value of the $k$ final test scores. This is your final (more reliable) test set prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "AC8gYVpi3GU6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5212710351670162"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_two(k):\n",
    "    from sklearn.datasets import load_boston\n",
    "    from sklearn.linear_model import Ridge\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.model_selection import KFold\n",
    "\n",
    "    X,y = load_boston(return_X_y=True)\n",
    "    X=X[:60,:]\n",
    "    y=y[:60]\n",
    "    alpha_list = [0.001,0.01,0.1,1,10]\n",
    "    \n",
    "    # Your code here\n",
    "    kf = KFold(n_splits=5, shuffle =True, random_state=0)\n",
    "\n",
    "    score_list =[]\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_training, X_test = X[train_index], X[test_index]\n",
    "        y_training, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        #further splitting the train data 75%/25%\n",
    "        X_train, X_val,y_train, y_val = train_test_split(X_training, y_training,\n",
    "                                                         test_size = 0.25, random_state =0)\n",
    "    \n",
    "        scores =0\n",
    "        best_alpha = 0\n",
    "        best_model = None\n",
    "        for alpha_val in alpha_list:\n",
    "            clf_ridge = Ridge(alpha = alpha_val).fit(X_train, y_train)\n",
    "            val_score = clf_ridge.score(X_val, y_val)       \n",
    "            if val_score > scores:\n",
    "                best_alpha = alpha_val\n",
    "                best_model = clf_ridge\n",
    "                \n",
    "        score_list.append(best_model.score(X_test, y_test)) \n",
    "   \n",
    "    mean_test_score = sum(score_list)/ len(score_list)\n",
    "        \n",
    "    \n",
    "    return mean_test_score\n",
    "    \n",
    "answer_two(3)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TMn9p67w3GU8"
   },
   "source": [
    "### Question 3 (30 points)  Decision trees.\n",
    "\n",
    "For this question, we'll work with the Statlog (German Credit Data) dataset that classifies people described by a set of attributes as good or bad credit risks. Download the dataset from [here](https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)) (Use the german.data). The last column is the prediction target and the remaining columns are features. \n",
    "\n",
    "\n",
    "\n",
    "(a) (20 points) First transform the categorical features into one-hot encodings. Then train a decision tree classifier and a random forest classifier. You should return 6 items as follows: the trained decision tree classifier, the trained random forest classifier, decision tree training accuracy, decision tree test accuracy,  random forest training accuracy, random forest test accuracy. **Please use random_state = 0 for train_test_split, decision tree classifier, and random forest classifier.**\n",
    "\n",
    "*Hint 1: The columns of categorical features are 0, 2, 3, 5, 6, 8, 9, 11, 13, 14, 16, 18, 19.*\n",
    "\n",
    "*Hint 2: You may have a problem using `OneHotEncoder` to handle string values and numerical values at the same time. You can transform the string columns first, and then concatenate with the numerical features.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "MlbJeDTS3GU8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DecisionTreeClassifier(max_depth=3, random_state=0),\n",
       " RandomForestClassifier(n_estimators=10, random_state=0),\n",
       " 0.7543391188251002,\n",
       " 0.724,\n",
       " 0.9879839786381842,\n",
       " 0.9879839786381842)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_three_a():\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "    #load the dataset\n",
    "    df = pd.read_csv('german.txt', sep=' ')\n",
    "    \n",
    "    target = '1.1'\n",
    "    X = df.drop(target, axis=1)\n",
    "    y= df[[target]]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "        # separate out numerical and string columns\n",
    "        # do transformation like one -hot encoding and scaling.\n",
    "        #  decision tree and random forest\n",
    "\n",
    "    cat_attribs = list(X_train.select_dtypes(exclude=[np.number]))\n",
    "\n",
    "    \n",
    "    encoder =ColumnTransformer([(\"encoding\", OneHotEncoder(),cat_attribs)], remainder=\"passthrough\")\n",
    "    train_X_prepared = encoder.fit_transform(X_train)\n",
    "    test_X_prepared = encoder.transform(X_test)\n",
    "\n",
    "    DT_classifier = DecisionTreeClassifier(max_depth = 3, random_state=0)\n",
    "    DT_classifier.fit(train_X_prepared, y_train)\n",
    "    train_score_DT = DT_classifier.score(train_X_prepared, y_train)\n",
    "    test_score_DT = DT_classifier.score(test_X_prepared, y_test)\n",
    "    \n",
    "    RF_classifier = RandomForestClassifier(n_estimators = 10, random_state=0)\n",
    "    RF_classifier.fit(train_X_prepared, y_train)\n",
    "    train_score_RF = RF_classifier.score(train_X_prepared, y_train)\n",
    "    test_score_RF = RF_classifier.score(test_X_prepared, y_test)\n",
    "    \n",
    "    return DT_classifier, RF_classifier, train_score_DT, test_score_DT, train_score_RF, train_score_RF\n",
    "answer_three_a()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train and test scores show that there is no major overfitting, so chosen hypermeters are sufficient for this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BGHnU1TVHUiX"
   },
   "source": [
    "(b) (10 points) For the Decision Tree Classifier, compute **feature importance** that comes with decision tree classifier in scikit-learn to get the top three most important features. Also do it for the random forest. Are they the same sets of features? Or do you have any interesting findings and comments?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "4q1z4UatHT6x"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 3, 54, 19]), array([55, 54,  3]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_three_b():\n",
    "    import numpy as np\n",
    "    \n",
    "    DT_classifier, RF_classifier, _,_,_,_ = answer_three_a()\n",
    "    \n",
    "    DT_important_features = np.argsort(DT_classifier.feature_importances_)[::-1][:3]\n",
    "    RF_important_features = np.argsort(RF_classifier.feature_importances_)[::-1][:3]\n",
    "\n",
    "    return DT_important_features, RF_important_features \n",
    "    # DT_important_features is np.array of 3 numbers indicates the indices of top 3 important features in the training features after onehot encoding.\n",
    "    # DT_important_features is np.array of 3 numbers indicates the indices of top 3 important features in the training features after onehot encoding.\n",
    "answer_three_b()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two out of three most important features are common are for both the classifiers but the most important feature at the top of the list in random forest classifier does not even show up in the list of top three for decision tree classifier. The order of the commons features is also out of order. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 (20 points) Data Leakage\n",
    " \n",
    "Suppose that you are helping the Ann Arbor Rock Music Club in organizing a concert in a week from now. They want you to predict the number of people who will attend this concert, so that they can arrange for drinks etc.\n",
    " \n",
    "You have access to historical data about attendance in previous concerts, so you decide to design this as a regression problem with the following features:\n",
    " \n",
    "1.   Day of the week\n",
    "2.   Time of the day\n",
    "3.   Number of tickets sold at the door\n",
    "4.   Average quality of musicians performing\n",
    "5.   Capacity of the venue\n",
    " \n",
    "Your model performs well on training, validation and test data, so you are confident that it will successfully predict the attendance in the upcoming concert.\n",
    " \n",
    "Is there a data leakage problem here? If so, what is the source?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "All the features provided can be found in the deployment dataset except for one, 'number of tickets sold at the door'. Tickets are sold at the door right at the time of and/or just before the concert begins. So, this data is collected after the concert and including it will result in better attendance prediction but it will be missing from our actual test set(deployment data) and essetially causes data leakage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "si670f21_hw_5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
