{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OWTe9VS3_b11"
   },
   "source": [
    "## SI 670 Applied Machine Learning, Week 5:  One-hot encoding, calibration, decision trees, random forests, data leakage. (Due Wednesday October 5 2022, 11:59pm.)\n",
    "\n",
    "Total: 100 points\n",
    "Question 1: 30 points\n",
    "Question 2: 20 points\n",
    "Question 3: 30 points \n",
    "Question 4: 20 points\n",
    "\n",
    "* This homework is worth 100 points in total. Correct answers and code receive full credit, but partial credit will be awarded if you have the right idea even if your final answers aren't quite right.\n",
    "\n",
    "* Submit your completed notebook file to the Canvas site - **IMPORTANT**: please name your submitted file `si670f22-hw5-youruniqname.ipynb`\n",
    "\n",
    "* Any file submitted after the deadline will be marked as late. Please consult the syllabus regarding late submission policies. You can submit the homework as many time as you want, but only your latest submission will be graded.\n",
    "\n",
    "* As a reminder, the notebook code you submit must be your own work. Feel free to discuss general approaches to the homework with classmates. If you end up forming more of a team discussion on multiple questions, please include the names of the people you worked with at the top of your notebook file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8E6z1mGZ3GU1"
   },
   "source": [
    "### Question 1 (30 points)\n",
    "\n",
    "This question doesn't require coding, in the sense of writing programs that do all the computing of the answer for you: you can figure out the answer by hand and then put your answers into the notebook by defining the correct numpy array in python. Alternatively you could write out your answers in markdown using simple LaTeX tags. (See [here](https://csrgxtu.github.io/2015/03/20/Writing-Mathematic-Fomulars-in-Markdown/) for how to write things with math operators in LaTeX format.)\n",
    "\n",
    "\n",
    "#### (a) (10 points) One-hot encoding\n",
    "\n",
    "If you have a dataset with three data points, and each data point has three features. Among them, X2 and X3 are categorical variables:\n",
    "\n",
    "|    X1\t|  X2 \t|  X3\t|\n",
    "|----\t|----\t|----\t|\n",
    "|   1.3\t|  a \t| a \t|\n",
    "|   0.7 |  b \t| c \t|\n",
    "|   2.1 |  a \t| b     |\n",
    "\n",
    "X2 can take up the values a and b, and X3 can take up the values a, b and c.\n",
    "\n",
    "Please manually convert this dataset into numerical format with the categorical variables transformed into one-hot encoding. Please keep the order of X1, X2, and X3, and use alphabetical order for the one-hot encoding.\n",
    "\n",
    "\n",
    "#### (b) (10 points) Calibration\n",
    "Recall the calibration curve has the predicted probability as its x-axis and the true probability (also known as the \"empirical probability\" as its y-axis. Suppose you are given a binary classifier and its predicted probabilities on a test set with 15 data points. The labels of these data points are also given. Please calculate the true probabilities in three bins: \\[0, 0.3), \\[0.3, 0.7), \\[0.7, 1\\]. You could further use these probabilities to draw a calibration curve but it's not required for this question. You only need to give the 3 numbers indicating the true probabilities for each bin.  It might help to recall the weather example from class: the \"true\" empirical probability of rain is just the fraction of times it *actually* rained according to the data, for a given predicted probability (or range of probabilities) from the weatherperson.\n",
    "\n",
    "|Predicted probability | Label |\n",
    "|----\t               |----   |\n",
    "|   0.40               |   0   |\n",
    "|   0.77               |   1   |\n",
    "|   0.84               |   0   |\n",
    "|   0.68               |   0   |\n",
    "|   0.73               |   1   |\n",
    "|   0.88               |   1   |\n",
    "|   0.69               |   0   |\n",
    "|   0.24               |   0   |\n",
    "|   0.70               |   1   |\n",
    "|   0.41               |   1   |\n",
    "|   0.34               |   1   |\n",
    "|   0.18               |   1   |\n",
    "|   0.31               |   1   |\n",
    "|   0.58               |   1   |\n",
    "|   0.00               |   0   |\n",
    "\n",
    "\n",
    "#### (c) (10 points) Random forest parameters\n",
    "\n",
    "Suppose your current random forest classifier is facing an overfitting situation. Please state whether increasing or decreasing the following parameters can potentially help you reduce overfitting or not, and why?\n",
    "\n",
    "(i) `n_estimators`\n",
    "\n",
    "(ii) `max_features`\n",
    "\n",
    "(iii) `max_depth`\n",
    "\n",
    "(iv) `n_jobs`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.3, 0.7, 2.1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "df = np.array([1.3, 0.7, 2.1])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (a) \n",
    "#just create columns X2_a and X2_b and then X3_a,X3_b and X3_c. Drop one column because of correlation\n",
    "\n",
    "#replace {a :1, b:0} and rename column to X2_a\n",
    "#create new columns X3_a,X3_b and enter value 1 in X3_a if the value in X3 is a otherwise 0, do the same \n",
    "# for X3_b and then just drop X3\n",
    "\n",
    "# X1 = np.array(1.3, 0.7, 2.1)\n",
    "# df = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**(c)**\n",
    "(i) `n_estimators`: Increasing the value of this parameters reduces overfitting for larger datasets because on a small dataset there is only so many diferent trees we can fit till the prediction becomes stable but for a larger dataset the prediction is an average of the result produced by the different trees and hence, even if a individual tree is overfitting, the result is derived from combining result from all different trees. So, more the no.of decision trees more stable will be our prediction.\n",
    "\n",
    "(ii) `max_features`: We want max-features to not be very small not a be the whole n-features because an optimum value will allow for the trees to be diverse, thus making the average prediction more meaningful. If we use very less features that might lead to inefficient tree (not able to partition properly) and if we use all the features, we will get similar trees and mit can lead to overfitting. So, the relationship is non-linear.\n",
    "\n",
    "(iii) `max_depth` : Max depth does not effect overfitting in case of random forests unlike decision trees, because even if one tree overfits not all of them would, so the average prediction does not result in overfitting. \n",
    "\n",
    "(iv) `n_jobs`: This parameter is used for parallelization and does not effect overfitting in any way. More processors which just make the model run faster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9Qlj8Dm3GU5"
   },
   "source": [
    "### Question 2 (20 points) Cross-validation for very small datasets.\n",
    "\n",
    "In our lecture about data leakage, we talked about one simple strategy to help you avoid data leakage: before you do any work with a new dataset, split off a final test dataset, and use this final test dataset as the very last step in your validation. However, when you have a very small dataset, one issue is that it leads to really tiny test sets, which leads to unreliable test evaluation scores. For example, if you have a dataset with a total of 60 samples, and hold back 25% as a final test set, you'll get a final test set with 15 samples. In this case, a single evaluation score based on merely 15 samples could be very unreliable and probably not something to be relied on heavily.\n",
    "\n",
    "To make evaluation more reliable, we discussed how people usually use *cross-validation* to generate *multiple* evaluation scores, each on a different train/test split of the data. That is, you split the train and test set multiple times and then calculate the average of the resulting test scores. This is the approach we'll use to estimate a more reliable final test set score.  We don't want to use these final test sets to also tune our hyperparameters (to avoid data leakage), so we make sure to learn the model and tune the hyperparameters using only the data in the training split.  To do that, we do a second split *within the training data split* so that we have (i) a training set for the model and (ii) a separate *validation split* that's used to evaluate the model and pick the best setting for the hyperparameters.\n",
    "\n",
    "Here's the general recipe:\n",
    "\n",
    "1. Split the whole dataset into $k$ equal folds\n",
    "2. For $i$ from 1 to $k$   (for each of the $k$ folds)\n",
    "\n",
    "    a. Take the $i$-th fold as a final test set. \n",
    "    \n",
    "    b. With the remaining data (i.e. combining the other folds), apply a standard train/test split (75%/25%).\n",
    "    \n",
    "    c. For each possible tuning value of hyperparameter (in our case, alpha):\n",
    "    \n",
    "       - Train the model with the 75% part using a specific hyperparameter choice (for alpha)\n",
    "             \n",
    "       - Evaluate the model with the 25% part. This 25% is our \"validation set\".\n",
    "             \n",
    "       Pick the hyperparameter value that gave the best score on the validation set.\n",
    "            \n",
    "    d. Once you find the optimal hyperparameter, do the test set predictions on this $i$-th fold and calculate the test score for this $i$-th fold.\n",
    "  \n",
    "3. Report the average of the final test set scores you got across all $k$ folds.\n",
    "\n",
    "To simulate a small dataset scenario, we have provided the code that selects the first 60 samples from the built-in boston dataset.. We've also given you the variable 'alpha_list', which has the range of ridge regression hyperparameter alphe you should use for tuning. \n",
    "\n",
    "Write the code that implements the above scheme on this subset of the boston dataset. You can split the whole dataset into k folds (step 1) by using the handy `KFold` function (https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html). For each fold, you can then use the default `train_test_split` to get that's fold's training and validation data (step 2b). With this training data, train a Ridge regression model, and use the validation set to evaluate, tuning to find the optimal hyper-parameter alpha value within each fold (step 2c). Get the final test set score using this optimal model (step 2d). Repeat for all $k$ folds to obtain a set of final test scores.\n",
    "\n",
    "Finally, you need to return the mean value of the $k$ final test scores. This is your final (more reliable) test set prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this case special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows:\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and:\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "    \n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 0.7697830928625822\n",
      "10 0.5766129330846955\n",
      "10 0.7172525174722777\n",
      "10 0.8077392115669034\n",
      "10 0.61397744411235\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6489744786914632, 0.5212710351670162)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "X,y = load_boston(return_X_y=True)\n",
    "X=X[:60,:]\n",
    "y=y[:60]\n",
    "alpha_list = [0.001,0.01,0.1,1,10]\n",
    "    \n",
    "# Your code here\n",
    "kf = KFold(n_splits=5, shuffle =True, random_state=0)\n",
    "a =kf.get_n_splits(X)\n",
    "count = 0\n",
    "score_list =[]\n",
    "score_list_2 =[]\n",
    "for train_index, test_index in kf.split(X):\n",
    "    \n",
    "#     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "#     print(\"size:\",train_index.shape, test_index.shape )\n",
    "    X_training, X_test = X[train_index], X[test_index]\n",
    "    y_training, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    #further splitting the train data 75%/25%\n",
    "    X_train, X_val,y_train, y_val = train_test_split(X_training, y_training, test_size = 0.25, random_state =0)\n",
    "    \n",
    "    scores =0\n",
    "    best_alpha = 0\n",
    "    best_model_2 = None\n",
    "    for alpha_val in alpha_list:\n",
    "        clf_ridge = Ridge(alpha = alpha_val).fit(X_train, y_train)\n",
    "        val_score = clf_ridge.score(X_val, y_val)       \n",
    "        if val_score > scores:\n",
    "            best_alpha = alpha_val\n",
    "            best_model_2 = clf_ridge\n",
    "    print(best_alpha, val_score)\n",
    "            \n",
    "    score_list_2.append(best_model_2.score(X_test, y_test)) \n",
    "    \n",
    "    \n",
    "#     score_list.append(best_model.score(X_test, y_test))\n",
    "\n",
    "    # trying all different alpha values\n",
    "    ridge = Ridge()\n",
    "    parameters = {'alpha':alpha_list}\n",
    "    clf = GridSearchCV(ridge, parameters)\n",
    "#     ridge_clf = clf.fit(X_train, y_train)\n",
    "    ridge_clf = clf.fit(X_training, y_training)\n",
    "    best_alpha =  ridge_clf.cv_results_['params'][ridge_clf.best_index_]['alpha']\n",
    "#     print(best_alpha, ridge_clf.score(X_test, y_test))\n",
    "    best_model = ridge_clf.best_estimator_\n",
    "\n",
    "#     score_list.append(best_model.score(X_val, y_val))\n",
    "    score_list.append(best_model.score(X_test, y_test))\n",
    "        \n",
    "\n",
    "sum(score_list)/ len(score_list), sum(score_list_2)/ len(score_list_2)\n",
    "# score_list, score_list_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "AC8gYVpi3GU6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43\n",
      " 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59] TEST: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 40 41 42 43\n",
      " 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59] TEST: [20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39]\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39] TEST: [40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59]\n"
     ]
    }
   ],
   "source": [
    "def answer_two(k):\n",
    "    from sklearn.datasets import load_boston\n",
    "    from sklearn.linear_model import Ridge\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.model_selection import KFold\n",
    "\n",
    "    X,y = load_boston(return_X_y=True)\n",
    "    X=X[:60,:]\n",
    "    y=y[:60]\n",
    "    alpha_list = [0.001,0.01,0.1,1,10]\n",
    "    \n",
    "    # Your code here\n",
    "    kf = KFold(n_splits=5, shuffle =True, random_state=0)\n",
    "\n",
    "    score_list =[]\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_training, X_test = X[train_index], X[test_index]\n",
    "        y_training, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "    \n",
    "    return #mean_test_score\n",
    "    \n",
    "answer_two(3)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TMn9p67w3GU8"
   },
   "source": [
    "### Question 3 (30 points)  Decision trees.\n",
    "\n",
    "For this question, we'll work with the Statlog (German Credit Data) dataset that classifies people described by a set of attributes as good or bad credit risks. Download the dataset from [here](https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)) (Use the german.data). The last column is the prediction target and the remaining columns are features. \n",
    "\n",
    "\n",
    "\n",
    "(a) (20 points) First transform the categorical features into one-hot encodings. Then train a decision tree classifier and a random forest classifier. You should return 6 items as follows: the trained decision tree classifier, the trained random forest classifier, decision tree training accuracy, decision tree test accuracy,  random forest training accuracy, random forest test accuracy. **Please use random_state = 0 for train_test_split, decision tree classifier, and random forest classifier.**\n",
    "\n",
    "*Hint 1: The columns of categorical features are 0, 2, 3, 5, 6, 8, 9, 11, 13, 14, 16, 18, 19.*\n",
    "\n",
    "*Hint 2: You may have a problem using `OneHotEncoder` to handle string values and numerical values at the same time. You can transform the string columns first, and then concatenate with the numerical features.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 999 entries, 0 to 998\n",
      "Data columns (total 21 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   A11     999 non-null    object\n",
      " 1   6       999 non-null    int64 \n",
      " 2   A34     999 non-null    object\n",
      " 3   A43     999 non-null    object\n",
      " 4   1169    999 non-null    int64 \n",
      " 5   A65     999 non-null    object\n",
      " 6   A75     999 non-null    object\n",
      " 7   4       999 non-null    int64 \n",
      " 8   A93     999 non-null    object\n",
      " 9   A101    999 non-null    object\n",
      " 10  4.1     999 non-null    int64 \n",
      " 11  A121    999 non-null    object\n",
      " 12  67      999 non-null    int64 \n",
      " 13  A143    999 non-null    object\n",
      " 14  A152    999 non-null    object\n",
      " 15  2       999 non-null    int64 \n",
      " 16  A173    999 non-null    object\n",
      " 17  1       999 non-null    int64 \n",
      " 18  A192    999 non-null    object\n",
      " 19  A201    999 non-null    object\n",
      " 20  1.1     999 non-null    int64 \n",
      "dtypes: int64(8), object(13)\n",
      "memory usage: 164.0+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('german.txt', sep=' ')\n",
    "# df.select_dtypes(exclude = ['object'])\n",
    "df.info(), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['1.1'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['A11', '6', 'A34', 'A43', '1169', 'A65', 'A75', '4', 'A93', 'A101',\n",
       "       '4.1', 'A121', '67', 'A143', 'A152', '2', 'A173', '1', 'A192', 'A201',\n",
       "       '1.1'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 13)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "target = '1.1'\n",
    "X = df.drop(target, axis=1)\n",
    "y= df[[target]]\n",
    "num_cols = X.select_dtypes('number').columns\n",
    "cat_cols = X.select_dtypes(exclude= np.number).columns\n",
    "len(X.select_dtypes(exclude= np.number).columns), len([0, 2, 3, 5, 6, 8, 9, 11, 13, 14, 16, 18, 19])\n",
    "# len(num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>780</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>835</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>749 rows Ã— 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0    1    2    3    4    5    6    7    8    9   ...   44   45   46  \\\n",
       "780  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  1.0  1.0  ...  1.0  0.0  0.0   \n",
       "253  0.0  1.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  ...  1.0  0.0  0.0   \n",
       "363  1.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  ...  1.0  0.0  1.0   \n",
       "803  0.0  1.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  1.0  ...  0.0  0.0  1.0   \n",
       "817  1.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  ...  1.0  0.0  0.0   \n",
       "..   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "835  0.0  0.0  0.0  1.0  0.0  0.0  1.0  0.0  0.0  0.0  ...  1.0  0.0  0.0   \n",
       "192  0.0  0.0  0.0  1.0  0.0  0.0  1.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "629  1.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  ...  1.0  0.0  0.0   \n",
       "559  1.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  ...  0.0  1.0  0.0   \n",
       "684  0.0  0.0  0.0  1.0  0.0  0.0  1.0  0.0  0.0  1.0  ...  0.0  1.0  0.0   \n",
       "\n",
       "      47   48   49   50   51   52   53  \n",
       "780  0.0  1.0  0.0  0.0  1.0  1.0  0.0  \n",
       "253  0.0  1.0  0.0  0.0  1.0  1.0  0.0  \n",
       "363  0.0  0.0  0.0  1.0  0.0  1.0  0.0  \n",
       "803  0.0  0.0  0.0  1.0  0.0  1.0  0.0  \n",
       "817  0.0  0.0  1.0  1.0  0.0  1.0  0.0  \n",
       "..   ...  ...  ...  ...  ...  ...  ...  \n",
       "835  0.0  1.0  0.0  1.0  0.0  1.0  0.0  \n",
       "192  0.0  1.0  0.0  1.0  0.0  1.0  0.0  \n",
       "629  0.0  1.0  0.0  1.0  0.0  1.0  0.0  \n",
       "559  0.0  1.0  0.0  0.0  1.0  1.0  0.0  \n",
       "684  0.0  1.0  0.0  0.0  1.0  1.0  0.0  \n",
       "\n",
       "[749 rows x 54 columns]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "# X_train[cat_cols.columns] = OneHotEncoder().fit_transform(X_train[cat_cols])\n",
    "# X_train[num_cols.columns] = StandardScaler().fit_transform(X_train[num_cols])\n",
    "# X_test[cat_cols.columns] =X_test_encoded = OneHotEncoder().fit_transform(X_test[cat_cols])\n",
    "# X_test[num_cols.columns] = StandardScaler().fit_transform(X_test[num_cols])\n",
    "\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# clf = DecisionTreeClassifier(max_features='sqrt', random_state=0)\n",
    "\n",
    "# clf.fit(X_train, y_train)\n",
    "# clf.score(X_test, y_test)\n",
    "\n",
    "# X_train_encoded.categories_, #X_test_encoded.categories_\n",
    "OH_cols_train = pd.DataFrame(OneHotEncoder(handle_unknown='ignore', sparse=False).fit_transform(X_train[cat_cols]))\n",
    "OH_cols_train.index = X_train.index\n",
    "# numeric_X_train = X_train.drop(cat_cols, axis=1)\n",
    "# X_train[cat_cols]= \n",
    "# X_train[num_cols] = StandardScaler().fit_transform(X_train[num_cols])\n",
    "OH_cols_train\n",
    "# OneHotEncoder().fit_transform(X_train[cat_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    " from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "df = pd.read_csv('german.txt', sep=' ')\n",
    "    \n",
    "target = '1.1'\n",
    "X = df.drop(target, axis=1)\n",
    "y= df[[target]]\n",
    "\n",
    "num_attribs = list(X_train.select_dtypes(include=[np.number]))\n",
    "cat_attribs = list(X_train.select_dtypes(exclude=[np.number]))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "full_pipeline = ColumnTransformer([\n",
    "            (\"num\", StandardScaler(), num_attribs),\n",
    "            (\"cat\", OneHotEncoder(), cat_attribs),\n",
    "        ])\n",
    "\n",
    "train_X_prepared = full_pipeline.fit_transform(X_train)\n",
    "test_X_prepared = full_pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A11',\n",
       " 'A34',\n",
       " 'A43',\n",
       " 'A65',\n",
       " 'A75',\n",
       " 'A93',\n",
       " 'A101',\n",
       " 'A121',\n",
       " 'A143',\n",
       " 'A152',\n",
       " 'A173',\n",
       " 'A192',\n",
       " 'A201']"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(X_train.select_dtypes(include=[np.number]))\n",
    "list(X_train.select_dtypes(exclude=[np.number]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "id": "MlbJeDTS3GU8"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'category_encoders'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [172]\u001b[0m, in \u001b[0;36m<cell line: 64>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m     train_score_RF \u001b[38;5;241m=\u001b[39m RF_classifier\u001b[38;5;241m.\u001b[39mscore(test_X_prepared, y_test)\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DT_classifier, RF_classifier, train_score_DT, test_score_DT, train_score_RF, train_score_RF\n\u001b[0;32m---> 64\u001b[0m \u001b[43manswer_three_a\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [172]\u001b[0m, in \u001b[0;36manswer_three_a\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestClassifier\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcategory_encoders\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mce\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#load the downloaded dataset\u001b[39;00m\n\u001b[1;32m     12\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgerman.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'category_encoders'"
     ]
    }
   ],
   "source": [
    "def answer_three_a():\n",
    "    import numpy as np\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    import category_encoders as ce\n",
    "\n",
    "    #load the downloaded dataset\n",
    "    df = pd.read_csv('german.txt', sep=' ')\n",
    "    \n",
    "    target = '1.1'\n",
    "    X = df.drop(target, axis=1)\n",
    "    y= df[[target]]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "        # separate out numerical and string columns\n",
    "        # do transformation like one -hot encoding and scaling.\n",
    "        # tran decision tree and random forest\n",
    "\n",
    "\n",
    "    num_attribs = list(X_train.select_dtypes(include=[np.number]))\n",
    "    cat_attribs = list(X_train.select_dtypes(exclude=[np.number]))\n",
    "    \n",
    "    \n",
    "\n",
    "    full_pipeline = ColumnTransformer([\n",
    "            (\"num\", StandardScaler(), num_attribs),\n",
    "            (\"cat\", OneHotEncoder(), cat_attribs),\n",
    "        ])\n",
    "\n",
    "#     train_X_prepared = full_pipeline.fit_transform(X_train)\n",
    "#     test_X_prepared = full_pipeline.transform(X_test)\n",
    "    \n",
    "#     train_X_prepared = X_train\n",
    "#     test_X_prepared = X_test\n",
    "\n",
    "    encoder = ce.OneHotEncoder(cat_attribs)\n",
    "    train_X_encoded = encoder.fit_transform(X_train)\n",
    "    test_X_encoded = encoder.transform(X_test)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    train_X_prepared = scaler.fit_tranform(X_train)\n",
    "    test_X_prepared = scaler.transform(X_test)\n",
    "\n",
    "    DT_classifier = DecisionTreeClassifier(max_depth=5, random_state=0)\n",
    "    DT_classifier.fit(train_X_prepared, y_train)\n",
    "    DT_y_pred_train = DT_classifier.predict(train_X_prepared)\n",
    "    DT_y_pred_test = DT_classifier.predict(test_X_prepared)\n",
    "    train_score_DT = accuracy_score(y_train, DT_y_pred_train)\n",
    "    test_score_DT = accuracy_score(y_test, DT_y_pred_test)\n",
    "#     cross_val_score(DT_classifier, train_X_prepared, y_train)\n",
    "    \n",
    "    \n",
    "    \n",
    "    RF_classifier = RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1, random_state=0)\n",
    "    RF_classifier.fit(train_X_prepared, y_train)\n",
    "    train_score_RF = RF_classifier.score(train_X_prepared, y_train)\n",
    "    train_score_RF = RF_classifier.score(test_X_prepared, y_test)\n",
    "    \n",
    "    return DT_classifier, RF_classifier, train_score_DT, test_score_DT, train_score_RF, train_score_RF\n",
    "answer_three_a()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BGHnU1TVHUiX"
   },
   "source": [
    "(b) (10 points) For the Decision Tree Classifier, compute **feature importance** that comes with decision tree classifier in scikit-learn to get the top three most important features. Also do it for the random forest. Are they the same sets of features? Or do you have any interesting findings and comments?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cc/m0qhb3297hl_fmwwqxrf14c00000gn/T/ipykernel_735/1253243345.py:46: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  RF_classifier.fit(train_X_prepared, y_train)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(61, 749, (749, 61), (749, 20))"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DT_classifier, RF_classifier, _,_,_,_ = answer_three_a()\n",
    "len(DT_classifier.feature_importances_),len(train_X_prepared), train_X_prepared.shape, X_train.shape\n",
    "# len(RF_classifier.feature_importances_),len(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "id": "4q1z4UatHT6x"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cc/m0qhb3297hl_fmwwqxrf14c00000gn/T/ipykernel_735/1453427024.py:46: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  RF_classifier.fit(train_X_prepared, y_train)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of values (61) does not match length of index (20)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [143]\u001b[0m, in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DT_important_features, RF_important_features , sorted_indices, labels, DT\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# DT_important_features is np.array of 3 numbers indicates the indices of top 3 important features in the training features after onehot encoding.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# DT_important_features is np.array of 3 numbers indicates the indices of top 3 important features in the training features after onehot encoding.\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[43manswer_three_b\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [143]\u001b[0m, in \u001b[0;36manswer_three_b\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     DT_classifier, RF_classifier, _,_,_,_ \u001b[38;5;241m=\u001b[39m answer_three_a()\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# your code here\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# there is a function which gives feature importance and another which visualises it\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     DT_important_features \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSeries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDT_classifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_importances_\u001b[49m\u001b[43m,\u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msort_values(ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#     DT = DT_classifier.feature_importances_[:8]\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#     sorted_indices = np.argsort(DT_important_features)[::-1] \u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#     labels = df.columns[1:]\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#     print(DT_classifier.feature_names_in_)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     RF_important_features \u001b[38;5;241m=\u001b[39m RF_classifier\u001b[38;5;241m.\u001b[39mfeature_importances_[:\u001b[38;5;241m3\u001b[39m]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/series.py:430\u001b[0m, in \u001b[0;36mSeries.__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    428\u001b[0m     index \u001b[38;5;241m=\u001b[39m ibase\u001b[38;5;241m.\u001b[39mdefault_index(\u001b[38;5;28mlen\u001b[39m(data))\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(data):\n\u001b[0;32m--> 430\u001b[0m     \u001b[43mcom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequire_length_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;66;03m# create/copy the manager\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, (SingleBlockManager, SingleArrayManager)):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/common.py:531\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    527\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;124;03mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[0;32m--> 531\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    532\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    534\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match length of index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    535\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    536\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values (61) does not match length of index (20)"
     ]
    }
   ],
   "source": [
    "def answer_three_b():\n",
    "    DT_classifier, RF_classifier, _,_,_,_ = answer_three_a()\n",
    "    \n",
    "    # your code here\n",
    "    # there is a function which gives feature importance and another which visualises it\n",
    "    DT_important_features = pd.Series(DT_classifier.feature_importances_,index=X_train.columns).sort_values(ascending=False).head(3)\n",
    "#     DT = DT_classifier.feature_importances_[:8]\n",
    "#     sorted_indices = np.argsort(DT_important_features)[::-1] \n",
    "#     labels = df.columns[1:]\n",
    "#     print(DT_classifier.feature_names_in_)\n",
    "    RF_important_features = RF_classifier.feature_importances_[:3]\n",
    "\n",
    "    return DT_important_features, RF_important_features , sorted_indices, labels, DT\n",
    "    # DT_important_features is np.array of 3 numbers indicates the indices of top 3 important features in the training features after onehot encoding.\n",
    "    # DT_important_features is np.array of 3 numbers indicates the indices of top 3 important features in the training features after onehot encoding.\n",
    "answer_three_b()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 (20 points) Data Leakage\n",
    " \n",
    "Suppose that you are helping the Ann Arbor Rock Music Club in organizing a concert in a week from now. They want you to predict the number of people who will attend this concert, so that they can arrange for drinks etc.\n",
    " \n",
    "You have access to historical data about attendance in previous concerts, so you decide to design this as a regression problem with the following features:\n",
    " \n",
    "1.   Day of the week\n",
    "2.   Time of the day\n",
    "3.   Number of tickets sold at the door\n",
    "4.   Average quality of musicians performing\n",
    "5.   Capacity of the venue\n",
    " \n",
    "Your model performs well on training, validation and test data, so you are confident that it will successfully predict the attendance in the upcoming concert.\n",
    " \n",
    "Is there a data leakage problem here? If so, what is the source?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of tickets sold at the door"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "si670f21_hw_5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
